\newpage
\chapter{Introduction}

\section{Context}
Meetings have become more and more essential in the workplace in order to exchange information and to make decisions. It has recently become possible to save meeting information including videos, audio files, transcripts and slides in multimedia archives of meeting recordings so that humans can find relevant information from past meetings, for instance, using tools such as meeting browsers \cite{popescubelis:tbe}.  A meeting browser can be defined as follows: \textit{A meeting browser is a system that enables a user to navigate around an archive of meetings, efficiently viewing and accessing the full multimodal content, based on automatic annotation, structuring and indexing of those information streams} \cite{mccowan2005amc}. 

One approach to meeting browsing is to design general-purpose meeting browsers that help users to locate the information that is searched for \cite{lalanne2005imm}, for instance, the meeting browser named Archivus at the University of Geneva and at the EPFL \cite{lisowska2004asa}, Ferret in Idiap Research Institute \cite{wellner2004brm}, Transcript-based Query and Browsing Interface (TQB) at the University of Geneva \cite{popescubelis:tam}, etc. However, another possibility is to design browsers that locate information automatically, for instance for verification (fact checking) purposes.

\section{Goal }

The goal of this project is to design an automatic browser following a question-answering approach, and to assess its performance on a set of pairs of true-false statements, which have been initially used to evaluate human-directed browsers.

In other words, the goal is to design and to implement a system that determines automatically the true and the false statement in each pair based on searching facts on meeting transcripts, to evaluate its performance over a set of about two hundreds such pairs for two recorded meetings, and to compare it with human subjects who used meeting browsers. A comparative analysis of the system and the human scores on specific questions should indicate whether or not system and humans have the same difficulties answering such questions. This work will thus show whether such a system should be developed as a full automatic browser that gives an exact answer for user's question or only help users locate relevant information in meeting recordings (thus, functioning as an assistant tool).

The pairs of true-false statements used for this system were created using a well-defined methodology to capture facts related to a meeting for the Browser Evaluation Test method, namely the BET questions \cite{BET}. The BET method and the BET questions are presented in detail  in the section 3.1.






% Definir clairement le sujet, l'objet du travail
% la question dans son context => same difficults for humans and automatic machine to answer BET questions?
% methode de travail, outil de la recherche, sources utilisees

\section{Approach}
Our proposed system is developed with a number of specific techniques that are suitable for the nature of the data and of the task. It proceeds in three stages as follows: 

The first stage is the pre-processing of the pair of BET questions and of the meeting transcript for the purpose of transforming them into an uniform data.

Then the second stage aims at identifying separately the passage of the transcript that is most likely to contain the answer for each question in a pair using a lexical similarity algorithm. For this, all passages in the transcript are compared with each other using passage scoring, which is a sum of scores of matched words between the passage and a question. Regarding the matched word score, it is computed based on a complex score of lexical similarity, which is not only based on matched words but also on the speaker of these words. This technique pays more attention to the features of a conversational document as meeting transcripts that the speaker of an utterance is always determined. 
 
Finally, the third stage compares two BET questions in the pair based on the two passages found for each question and hypothesizes that one statement is true if the score of its corresponding passage is higher than that of the other one. In case that they have the same scores, the distance between matched words for each question is used to give the answer for reason that the true statement has the smaller distance. 


\section{Evaluation methods}
The performance of this system is evaluated by answering about two hundred BET questions related to two meeting transcripts named IB4010 and IS1008c (see Chapter 3 for more details) from the AMI Meeting Corpus \cite{AMI_corpus}. Furthermore, the performance of the algorithm is also evaluated on ASR transcripts that were generated by Automatic Speech Recognition \cite{ASR_transcrips} as well as on meeting summaries based on ASR transcripts.
 
The last evaluation is performed by comparing BET scores obtained by human subjects with scores obtained by the system for the same BET questions. The BET scores by human subjects were obtained with the Transcript-based Query and Brower Interface (BET4TQB)\cite{popescubelis2007otm}, . This task is to answer the question whether the human subjects and the system have the same difficulties to answer such questions.   

Based on these results, the evaluations should help to provide information as to whether the system should be developed as a full automated system or as only an assistant tool that helps humans answer such type of questions.

% Presenter brievement le cheminement du travail (commenter les points saillants de la table des matieres)
\section{Structure of the report}
% The Introduction should better explain what BET exactly is!
This report contains 7 chapters. The first chapter is an introduction while the rest of the report provides detailed information. Chapter 2 reviews a number of available approaches that are widely applied in many question answering systems. Chapter 3 presents a brief description of the BET method as well as data used to test this system. Chapter 4 consists of three sections that describe the three main stages of our approach as mentioned above. Chapter 5 describes an evaluation method using reference answers in order to assess answers returned by the algorithm. Chapter 6 presents experiments on both manual and automatic (ASR) meeting transcripts. Moreover, at the end of chapter 6, we conduct a comparison and an evaluation of two specific areas: (i) BET human results are compared with those of the system in order to show limitations of an automatic answering as well as difficulties for both human and machine; (ii) the system and its questions are used to measure the quality of Automatic Speech Recognition (ASR) summaries. Finally, the last chapter provides the main conclusions and suggests specific directions for future research.
