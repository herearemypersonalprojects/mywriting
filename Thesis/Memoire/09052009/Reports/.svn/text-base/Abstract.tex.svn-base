
% Thesis Abstract -----------------------------------------------------


%\begin{abstractslong}    %uncommenting this line, gives a different abstract heading
\newpage
%\section*{Abstract}        %this creates the heading for the abstract page

\begin{abstract}
\addcontentsline{toc}{chapter}{Abstract}
\small

%1) Motivation/problem statement: Why do we care about the problem? What practical, scientific, theoretical or artistic gap is your research filling? 

% What is the system?
A system for automatic true-false question answering (QA) over meeting transcripts was developed using speaker-directed lexical similarity algorithm including n-grams matching. The main function of this system is to determine the true and false statement in a pair of complementary statements. These statements were created using a defined methodology about a fact related to the meeting for the Browser Evaluation Test method, namely BET questions \cite{BET}. Answering BET questions are done by human subjects using a meeting browser in order to evaluate the performance of this browser. Hence, this system is the first step to build an automatic assistant tool that helps humans answer such type of questions. 

%2) Methods/procedure/approach: What did you actually do to get your results? (e.g. analyzed 3 novels, completed a series of 5 oil paintings, interviewed 17 students)
Most question answering systems use lexical similarity algorithm to locate relevant passages most likely to contain the answer. For this, all passages are compared with each other using passage score that is sum of scores of matched words between question and passage. Word score may be calculated based on its frequency, its part of speech (PoS) or its relation with neighbour words. In our own algorithm, a simple lexical similarity algorithm is developed in which passage score is not only based on matched words but also on speaker of these words to retrieve the most relevant passage. This technique pays more attention to the features of a conversational document as meeting transcript. Based on the retrieved relevant passages, the system gives true-false answers.

%3) Results/findings/product: As a result of completing the above procedure, what did you learn/invent/create?
%Results
The performance of this system is evaluated by answering approximately two hundreds of BET questions, which were constructed by independent observers over two meetings of the AMI Meeting Corpus \cite{AMI_corpus}. Experimental results showed that around 58\% of retrieved passages are correct meanwhile the chance of guessing randomly one correct passage is less than 4\%. The proportion of correct answers finally achieved is around 61\%. This result is better than result of answering true-false questions by chance whose proportion of correct answers is only 50\%. In addition, the performance of the algorithm is also evaluated based on ASR transcripts, which were generated by an Automatic Speech Recognition \cite{ASR_transcrips}, as well as meeting summaries based on ASR transcripts by replacing original transcripts by them. These transcripts are certainly more \textit{noisy}. Thus, the proportion of correct answers reduces reasonably for passage retrieval. 

%4) Conclusion/implications:  What are the larger implications of your findings, especially for the problem/gap identified in step 1?

The last evaluation is performed by comparing BET scores by human subjects. These scores are from applying the BET method for Transcript-based Query and Brower Interface (BET4TQB)\cite{popescubelis2007otm} with scores obtainned by the system over the same BET questions. Our comparative analysis showed that human subjects generally answer questions that require a deduction better than an automatic question answering system. Furthermore, it should better develop this system as an assistant tool that helps humans answer such type of questions by locating the relevant passage rather than return the final answers. \\

Keywords: Question Answering, Meeting Browser Evaluation, Passage Retrieval, BET questions, True-False Answering, N-gram Matching, Lexical Similarity.

\normalsize
\begin{comment}
Test thu comment o day
va nhung cho khac
\end{comment}
\end{abstract}
%\end{abstracts}
%\end{abstractlongs}


% ---------------------------------------------------------------------- 
