
% Thesis Abstract -----------------------------------------------------


%\begin{abstractslong}    %uncommenting this line, gives a different abstract heading
\newpage
%\section*{Abstract}        %this creates the heading for the abstract page

\begin{abstract}
\addcontentsline{toc}{chapter}{Abstract}
\small

%1) Motivation/problem statement: Why do we care about the problem? What practical, scientific, theoretical or artistic gap is your research filling? 

% What is the system?
An automatic true-false question answering system over meeting transcripts was developed using a speaker-specific lexical similarity algorithm that includes n-grams matching. The main function of this system is to determine the true and false statement in a pair of complementary statements. These statements were created using a well-defined methodology to capture facts related to meetings as the Browser Evaluation Test method, namely BET \cite{BET}. Our system is the first attempt at building an automatic meeting browser. 

%2) Methods/procedure/approach: What did you actually do to get your results? (e.g. analyzed 3 novels, completed a series of 5 oil paintings, interviewed 17 students)
First of all, our system uses lexical similarity algorithms to locate passages that are most likely to contain the answer. For this, all passages are compared with each other using passage scoring. The passage scoring is calculated not only based on the sum of scores matched words between question and passage but also on the speaker of these words. This technique pays more attention to the features of a conversational document as meeting transcript. Based on the retrieved relevant passages, one question is considered to be true if the score of its corresponding passage is higher than that of the other question in the pair.

%3) Results/findings/product: As a result of completing the above procedure, what did you learn/invent/create?
%Results
The performance of this system is evaluated by answering approximately two hundred BET questions, which were constructed by independent observers during two meetings of the AMI Meeting Corpus \cite{AMI_corpus}. Experimental results show that around 58\% of retrieved passages are correct while the chance of randomly guessing one correct passage is less than 4\%. The proportion of correct answers finally achieved is around 61\%. This result is better than the result from answering true-false questions by chance whose proportion of correct answers is only 50\%. In addition, the performance of the algorithm is also evaluated on transcripts that were generated by Automatic Speech Recognition (ASR) , as well as on meeting summaries based on ASR transcripts. These transcripts are noisier and the proportion of correct answers decreases for passage retrieval. 

%4) Conclusion/implications:  What are the larger implications of your findings, especially for the problem/gap identified in step 1?

The last evaluation is performed by comparing BET scores obtained by human subjects with scores obtained by the system over the same BET questions. The BET scores by human subjects were obtained with Transcript-based Query and Brower Interface (BET4TQB)\cite{popescubelis2007otm}. A comparative analysis shows that human subjects generally answer questions that require a deduction better than an automatic question answering system does. 

In conclusion, this system should be integrated into existing meeting browsers as an assistant tool that helps humans answer such type of questions by locating the relevant passage rather than find the true-false answers. \\

Keywords: Question Answering, Meeting Browser Evaluation, Passage Retrieval, BET questions, True-False Answering, N-gram Matching, Lexical Similarity.

\normalsize
\begin{comment}
Test thu comment o day
va nhung cho khac
\end{comment}
\end{abstract}
%\end{abstracts}
%\end{abstractlongs}


% ---------------------------------------------------------------------- 
