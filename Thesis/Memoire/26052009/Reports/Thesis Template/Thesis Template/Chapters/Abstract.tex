
% Thesis Abstract -----------------------------------------------------


%\begin{abstractslong}    %uncommenting this line, gives a different abstract heading
%\begin{abstracts}        %this creates the heading for the abstract page

%1) Motivation/problem statement: Why do we care about the problem? What practical, scientific, theoretical or artistic gap is your research filling? 

% What is the system?
A system for automatic question answering over meeting transcripts was developed using speaker-directed lexical similarity algorithm including n-grams. The main function of this system is to determine the true and false statement in a pair of complementary statements. These statements were created using a defined methodology for the Browser Evaluation Test (BET questions) \cite{BET} about a fact related to the meeting. Answering BET questions was done before by human subjects using a meeting browser in order to evaluate the performance of this browser. Hence, this system is the first step to builds an automatic assistant tool that helps humans answer such type questions. That is also the purpose of this project.

%2) Methods/procedure/approach: What did you actually do to get your results? (e.g. analyzed 3 novels, completed a series of 5 oil paintings, interviewed 17 students)
Most question answering systems use lexical similarity algorithm, in which matched words between a question and a passage is assigned with various scores to locate relevant passages most likely to contain the answer. Word score may be calculated based on its frequency, its part of speech (PoS) or its relation with neighbour words. In this case, a simple lexical similaritly algorithm is developed in which passage score is not only based on matched words but also on speaker of these words. This technique pays more attention to the features of a conversational document as meeting transcripts. Based on retrieved passages, the system gives true-false answer.

%3) Results/findings/product: As a result of completing the above procedure, what did you learn/invent/create?
%Results
The performance of this system is evaluated by answering about two hundreds of BET questions, which were contructed by independant observers over two meetings of the AMI Corpus \href{http://corpus.amiproject.org/}{\texttt{http://corpus.amiproject.org/}}. Experimental results showed that around 58\% of retrieved passages are correct meanwhile the chance of guessing randomly one correct passage is less than 1\%. The proportion of correct answer achieved finally is around 61\%. This results is better than result of answering true-false question by chance which is 50\%. The algorithm is also evaluated on transcripts which were generated by an Automatic Speech Recognition (ASR transcripts). These transcripts are more "noise", hence the proportion of correct answers is 5\%-8\% lower than this from manual transcripts. 

%4) Conclusion/implications:  What are the larger implications of your findings, especially for the problem/gap identified in step 1?

A part of results are compared with those of human subjects which are from the BET for Transcript-based Query and Brower Interface (BET4TQB)\cite{popescubelis2008tot}. Our comparative analysis showed that an automatic system and human generally have the same difficulties in answering deductive-type questions. The system can be also used to measure the quality of a meeting summary by its scores compared with the score of original transcript, in which the summary is generated by an automatic summarizer based on ASR transcripts. \\

Keywords: Question Answering, Meeting Browser Evaluation, Passage Retrieval, BET questions, True-False Statement.
%\end{abstracts}
%\end{abstractlongs}


% ---------------------------------------------------------------------- 
