
% Thesis Abstract -----------------------------------------------------


%\begin{abstractslong}    %uncommenting this line, gives a different abstract heading
\newpage
%\section*{Abstract}        %this creates the heading for the abstract page

%\begin{abstract}
\section*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}
\small
%\singlespace
\onehalfspacing

%1) Motivation/problem statement: Why do we care about the problem? What practical, scientific, theoretical or artistic gap is your research filling? 

% What is the system?
An automatic true-false question answering system over meeting transcripts was developed using a speaker-directed lexical similarity algorithm that includes n-grams matching and lexical extensions. The main function of this system is to determine the true and false statement in a pair of complementary statements. These statements were created using a well-defined methodology to capture facts related to meetings as the Browser Evaluation Test method, namely BET \cite{BET}. Our system is the first attempt at building an automatic meeting browser. 

%2) Methods/procedure/approach: What did you actually do to get your results? (e.g. analyzed 3 novels, completed a series of 5 oil paintings, interviewed 17 students)
First of all, our system uses a lexical similarity algorithm to locate one passage that is the most likely to contain information about question. For this, all passages are compared with each other using passage scoring. The passage scoring is calculated not only based on the sum of scores of matched words between question and passage but also on the speaker of these words. Based on two retrieved passages corresponding to two question in the pair, one question is considered to be true if the score of its corresponding passage is higher than that of the other question.

%3) Results/findings/product: As a result of completing the above procedure, what did you learn/invent/create?
%Results
The performance of this system is evaluated by answering approximately two hundred BET questions, which were constructed by independent observers during two meetings of the AMI Meeting Corpus \cite{AMI_corpus}. Experimental results show that around 58\% of retrieved passages are correct while the chance of randomly guessing one correct passage is less than 4\%. The proportion of correct answers finally achieved is around 61\%. This result is better than the result from answering true-false questions by chance whose proportion of correct answers is only 50\%. In addition, the performance of the algorithm is also evaluated on transcripts that were generated by Automatic Speech Recognition (ASR) , as well as on meeting summaries based on ASR transcripts. These transcripts are noisier and the proportion of correct answers decreases for passage retrieval. The last evaluation is performed by comparing BET scores obtained by human subjects with scores obtained by the system over the same BET questions. The BET scores by human subjects were obtained with the Transcript-based Query and Brower Interface (BET4TQB)\cite{popescubelis2007otm}. A comparative analysis shows that human subjects generally answer questions that require a deduction better than an automatic question answering system does. 

In conclusion, this system should be integrated into existing meeting browsers as an assistant tool that helps humans answer such type of questions by locating the relevant passage rather than find the true-false answers. \\

\textbf{Keywords}: Question Answering, Meeting Browser Evaluation, Passage Retrieval, BET questions, N-gram Matching, Lexical Similarity.
\small

%\end{abstract}

\pagebreak

%\begin{abstract}
\section*{R\'esum\'e}

Un syst\`eme automatique de r\'eponse aux questions vrai-faux sur des transcriptions de r\'eunions a \'et\'e d\'evelopp\'e gr\^ace \`a un algorithme utilisant la similarit\'e lexicale. Cet algorithme inclut des extensions lexicales et n-grams. La fonction principale de ce syst\`eme est de d\'eterminer quel \'enonc\'e est vrai et quel \'enonc\'e est faux dans une paire de deux \'enonc\'es compl\'ementaires. Ces \'enonc\'es ont \'et\'e cr\'e\'es \`a l'aide d'une m\'ethodologie pr\'ecis\'ement d\'efinie, Browsers Evaluation Test \cite{BET}, pour capturer les faits d'int\'er\^et dans une r\'eunion. Notre syst\`eme repr\'esente la premi\`ere tentative de construction d'un logiciel de navigation automatique pour les enregistrements archiv\'es des r\'eunions.
Tout d'abord notre algorithme situe un passage de la transcription qui a le plus de chance de contenir des renseignements sur la question. Pour cela, tous les passages sont compar\'es entre eux gr\^ace \`a des scores qui sont calcul\'es non seulement sur la base des mots similaires entre la question et le passage, mais aussi en prenant en compte les locuteurs de ces mots. \`A partir des deux passages trouv\'es pour les deux questions dans la paire \`a d\'esambiguiser, une question est consid\'er\'ee comme vraie si le score de son passage correspondant est plus \'elev\'e que celui de l'autre question.

La performance de ce syst\`eme est \'evalu\'ee en r\'epondant \`a environ deux cent questions BET qui ont \'et\'e \'elabor\'ees par des observateurs ind\'ependants, pour deux r\'eunions du AMI Meeting Corpus \cite{AMI_corpus}. Les r\'esultats exp\'erimentaux montrent qu'environ 58\% des passages r\'ecup\'er\'es sont corrects tandis que la chance de deviner au hasard un passage correct est inf\'erieure \`a 4\%. La proportion de r\'eponses correctes \`a l'\'etat final est d'environ 61\%. Ce r\'esultat est sup\'erieur \`a celui que l'on peut obtenir par choix au hasard, dont la proportion de r\'eponses correctes est seulement de 50\%. En plus, le r\'esultat de l'algorithme est aussi \'evalu\'e sur les transcriptions qui ont obtenues par un syst\`eme de reconnaissance vocale (ASR, Automatic Speech Recognition), aussi bien que dans les r\'esum\'es de r\'eunions bas\'es sur ces m\^emes transcriptions. Ces transcriptions sont plus bruit\'ees et la proportion de r\'eponses correctes diminue pour la phase de la r\'ecup\'eration des passages pertinents. La derni\`ere \'evaluation est effectu\'ee en comparant les scores BET obtenus par des sujets humains avec ceux obtenus par le syst\`eme, pour les m\^emes questions BET. Les scores des humains sont obtenus gr\^ace au navigateur BET4TQB \cite{popescubelis2007otm}. Une analyse comparative montre que, en r\`egle g\'en\'erale, les humains r\'epondent mieux aux questions qui demandent une d\'eduction que le syst\`eme automatique ne le fait.

En conclusion, ce syst\'eme pourrait \^etre int\'egr\'e aux logiciels existants de navigateur des documents archiv\'es de r\'eunions comme un outil compl\'ementaire pour aider \`a trouver des r\'eponses \`a des questions de ce type en localisant le passage ad\'equat plut\^ot qu'en proposant directement de trouver la r\'eponse vraie ou fausse. 

\normalsize
\doublespace
%\end{abstract}



% ---------------------------------------------------------------------- 
